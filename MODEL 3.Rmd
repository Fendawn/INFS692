---
title: "MODEL 3"
author: "FENDAWN F. RECENTES"
date: '2022-12-13'
output:
  html_document: default
  pdf_document: default
---

# Helper Packages And Modeling Packages
```{r}
library(dplyr)    
library(ggplot2)   
library(stringr)  
library(cluster)    
library(factoextra)
library(gridExtra)  
library(tidyverse)
library(readr)
library(mclust)
library(tidyverse)
library(bestNormalize)
```


# We use the normalize radiomatics dataset here.
##  Radiomics data contains 197 rows and 431 columns
##  Failure.binary: binary property to predict


## Load and view radiomatics dataset

```{r}
radiomics = read_csv("C:\\Users\\MSU-TCTO OVCAA\\Documents\\normalRad.csv")
View(radiomics)
```

## To remove any missing value that might present in the data

```{r}
df <- na.omit(radiomics)
```


## We don't want our algorithms to depend to an arbitrary variable unit, we start by scaling/standardizing the data

```{r}
df <- scale(radiomics[c(3:431)])
head(df)
```

# K-Means Clustering

## K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters). The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized.


## We can compute k-means in R with the kmeans() function. Here will group the data into three clusters (centers = 3). The kmeans function has an nstart option that attempts multiple initial configurations and reports on the best one. Adding nstart = 25 will generate 25 initial configurations.

```{r}
k3 <- kmeans(df, centers = 3, iter.max = 100, nstart = 100)
str(k3)
```

## Print the result and we see that our groupings resulted in 3 cluster sizes of 44, 50, 103. We see the cluster centers (means) for the two groups across many variables starting from Failure, Entropy_cooc.W.ADC, GLNU_align.H.PET and so on. 

```{r}
k3
```

## fviz_cluster() will perform principal component analysis (PCA) and plot the data points according to the first three pricipal components that explain the majority of variance

```{r}
fviz_cluster(k3, data = df)
```


## Determining Optimal Clusters

## In the elbow method, the results suggest that 2 is the optimal number of clusters as it appears to be the bend in the knee or elbow.

## In the average silhouette method suggests that 2 clusters maximize the average silhouette values with 6 clusters coming in as second optimal number of clusters.

## In the gap statistic method which suggests 2 clusters as the optimal number of clusters.

```{r}
fviz_nbclust(df, kmeans, method = "wss") 
fviz_nbclust(df, kmeans, method = "silhouette")
fviz_nbclust(df, kmeans, method = "gap_stat") 
```

## Final analysis, extract and plot the results using 2 clusters

## With most of these approaches suggesting 2 as the number of optimal clusters, we perform the final analysis and extract the results using 2 clusters

```{r}
final <- kmeans(df, centers = 2, iter.max = 100, nstart = 100)
fviz_cluster(kmeans(df, centers = 2, iter.max = 100, nstart = 100), data = df)
```


## Visualize clusters using original variables

```{r}
clusters <- kmeans(df, centers = 3, iter.max = 100, nstart = 100)
radiomics <- radiomics |> mutate(cluster = clusters$cluster)
radiomics |> ggplot(aes(x = Failure, y = Entropy_cooc.W.ADC, col = as.factor(cluster))) + geom_point()
```


# Heirarchical Clustering

## Hierarchical clustering is an alternative approach to k-means clustering for identifying groups in the dataset. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendogram.

# Scaling/standardizing the data

```{r}
df <- radiomics %>%
  select_if(is.numeric) %>%  # select numeric columns
  select(-Failure.binary) %>%    # remove target column
  mutate_all(as.double) %>%  # coerce to double type
  scale()
```

## For Reproducibility

```{r}
set.seed(123)
```

## We compute the dissimilarity values with dist() and then feed these values into hclust().

```{r}
d <- dist(df, method = "euclidean")
```

## Hierarchical clustering using Complete Linkage

```{r}
hc1 <- hclust(d, method = "complete")
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 2, border = 1:4)
```

## Compute with Agnes

## Using Agnes function we can get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure). Based on the result, 0.8072963, this value is closer to 1 which may suggest strong clustering structure.

```{r}
set.seed(123)
hc2 <- agnes(df, method = "complete")
hc2$ac
pltree(hc2, cex = 0.6)
```

## Compute divisive hierarchical clustering using Diana

## The divise coefficient or amount of clustering structure found shows 0.7915983.

```{r}
hc4 <- diana(df)
hc4$dc
pltree(hc4, cex = 0.6)
```

## Ward's method

```{r}
hc5 <- hclust(d, method = "ward.D2" )
sub_grp <- cutree(hc5, k = 8)
table(sub_grp)
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 8, border = 2:5)
```

## Determining Optimal Clusters

## In the elbow method, the results suggest that 2 is the optimal number of clusters as it appears to be the bend in the knee or elbow.

## In the average silhouette method suggests that 2 clusters maximize the average silhouette values with 5 clusters coming in as second optimal number of clusters.

## In the gap statistic method which suggests 9 clusters as the optimal number of clusters.

```{r}
p1 <- fviz_nbclust(df, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
p2 <- fviz_nbclust(df, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
p3 <- fviz_nbclust(df, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```



# Model-Based

# Based from our result, we obtained a negative log-likelihood, negative BIC and ICL which means that a negative
number is lower in magnitude and is thus the "better" model.

```{r}
model3 <- Mclust(df[,1:10], G=3) 
summary(model3)
```

# Plot results

```{r}
plot(model3, what = "density") 
plot(model3, what = "uncertainty")
```


```{r}
legend_args <- list(x = "bottomright", ncol = 5)
plot(model3, what = 'BIC', legendArgs = legend_args)
plot(model3, what = 'classification')
plot(model3, what = 'uncertainty')
```


```{r}
probabilities <- model3$z 
colnames(probabilities) <- paste0('C', 1:3)

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)
```


```{r}
uncertainty <- data.frame(
  id = 1:nrow(df),
  cluster = model3$classification,
  uncertainty = model3$uncertainty
)
uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)
```



```{r}
cluster2 <- df %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = model3$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)
```



