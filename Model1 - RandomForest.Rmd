---
title: "MODEL 1 - Random Forest"
author: "FENDAWN F. RECENTES"
date: "12/16/2022"
output:
  pdf_document: default
  html_document: default
---

## Helper and Modeling Packages

```{r}
library(dplyr)    
library(ggplot2)  
library(ranger)   
library(h2o)    
library(readr)
library(rsample)
library(ROCR)
library(pROC)
```

```{r}
h2o.init()
```

## Load and view radiomics data set

```{r}
radiomics <- read_csv("C:\\Users\\MSU-TCTO OVCAA\\Documents\\normalRad.csv")
```

# DATA PREPARATION AND SPLITTING


# Split the data intro training (80%) and testing (20%) stratified in Failure.binary column

```{r}
set.seed(123)
split = initial_split(radiomics,prop = 0.8 ,strata = "Failure.binary")
radiomics_train <- training(split)
radiomics_test  <- testing(split)
```

# Convert target variable to a factor form

```{r}
radiomics$Failure.binary=as.factor(radiomics$Failure.binary)
```

# Number of features

```{r}
n_features <- length(setdiff(names(radiomics_train), "Failure.binary"))
```

## Train a default random forest model

```{r}
radiomics_rf1 <- ranger(
  Failure.binary ~ ., 
  data = radiomics_train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)
```

# Get OOB RMSE

```{r}
(default_rmse <- sqrt(radiomics_rf1$prediction.error))
```

# Create hyperparameter grid

```{r}
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)
```

# Execute full cartesian grid search

```{r cars}
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Failure.binary ~ ., 
    data            = radiomics_train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
```


# Assess top 10 models

```{r}
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)
```


```{r}
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

# Convert training data to h2o object

```{r}
train_h2o <- as.h2o(radiomics_train)
```

# Set the response column to Failure.binary

```{r}
response <- "Failure.binary"
```

# Set the predictor names

```{r}
predictors <- setdiff(colnames(radiomics_train), response)
```

```{r}
h2o_rf1 <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  ntrees = n_features * 10,
  seed = 123
)

h2o_rf1
```

# Hyperparameter grid

```{r}
hyper_grid <- list(
  mtries = floor(n_features * c(.05, .15, .25, .333, .4)),
  min_rows = c(1, 3, 5, 10),
  max_depth = c(10, 20, 30),
  sample_rate = c(.55, .632, .70, .80)
)
```

# Random grid search strategy

```{r}
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,   
  stopping_rounds = 10,         
  max_runtime_secs = 60*5      
)
```

# Perform grid search 

```{r}
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  ntrees = n_features * 10,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,           
  stopping_tolerance = 0.005,     
  search_criteria = search_criteria
)
```

# Collect the results and sort by our model performance metric of choice

```{r}
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)
random_grid_perf
```

# Re-run model with impurity-based variable importance

```{r}
rf_impurity <- ranger(
  formula = Failure.binary ~ ., 
  data = radiomics_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```

# Re-run model with permutation-based variable importance

```{r}
rf_permutation <- ranger(
  formula = Failure.binary ~ ., 
  data = radiomics_train, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```

# Print the top 20 features during Training

```{r}
p1 <- vip::vip(rf_impurity, num_features = 20, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 20, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)

```

## Compute predicted probabilities on training data

```{r}
m1_prob <- predict(h2o_rf1, train_h2o, type = "prob")
m1_prob <- as.data.frame(m1_prob)[,2]
train_h2o <- as.data.frame(train_h2o)
```

## Compute AUC metrics

```{r}
perf1 <- prediction(m1_prob,train_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
```

## Plot AUC

```{r}
plot(perf1, col = "black", lty = 2)
```

## Plot ROC curves

```{r}
roc(train_h2o$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
     percent=TRUE, col="black", lwd=2, print.auc=TRUE)
```

## Compute predicted probabilities on testing data

```{r}
test_h2o = as.h2o(testDF)
m2_prob <- predict(h2o_rf1, test_h2o, type = "prob")
m2_prob = as.data.frame(m2_prob)[,2]
test_h2o = as.data.frame(test_h2o)
```

## Compute AUC metrics

```{r}
perf2 <- prediction(m2_prob,test_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")
```

## Plot AUC

```{r}
plot(perf2, col = "black", lty = 2)
```

## ROC plot for testing data

```{r}
roc( test_h2o$Failure.binary ~ m2_prob, plot=TRUE, legacy.axes=FALSE, 
     percent=TRUE, col="black", lwd=2, print.auc=TRUE)
```
















